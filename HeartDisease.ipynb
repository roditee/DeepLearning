{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HeartDisease.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzIeeDh5P9CR",
        "outputId": "e6d68256-3fd7-42d0-fd20-a0f6756d2428"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset): \n",
        "  def __init__(self, phase='train'):\n",
        "   # 데이터 읽기\n",
        "   f = pd.read_csv('heart.csv')\n",
        "   data = f.to_numpy()\n",
        "   if phase=='train':\n",
        "     data=data[:220,:]\n",
        "   if phase=='val':\n",
        "     data=data[221:,:]\n",
        "\n",
        "   # 전처리\n",
        "   self.x_data = data[:,0:13]\n",
        "   self.x_data = ( self.x_data -  self.x_data.min(axis=0)) / ( self.x_data.max(axis=0) -  self.x_data.min(axis=0))\n",
        "   self.y_data = data[:, 13:14]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    x = torch.Tensor(self.x_data[idx])\n",
        "    y = torch.LongTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "\n",
        "batch_=10\n",
        "dataloader ={ 'train' :  DataLoader(CustomDataset(phase='train'), batch_size=batch_, shuffle=True),\n",
        "               'val' :  DataLoader(CustomDataset(phase='val'), batch_size=batch_, shuffle=False)\n",
        " }\n",
        "\n",
        "\n",
        "## Define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(13, 100)    \n",
        "        self.fc2 = nn.Linear(100, 50)   \n",
        "        self.fc3 = nn.Linear(50, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "model = Net() # 모델 구축\n",
        "loss_fn =  torch.nn.CrossEntropyLoss()  # 손실함수 \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 옵티마이저 매개변수 갱신\n",
        "\n",
        "nb_epochs = 100\n",
        "\n",
        "for epoch in range(0, nb_epochs):\n",
        "  avg_cost = 0\n",
        "  sample_size = 0\n",
        "  avg_cost_ = 0\n",
        "  sample_size_ = 0\n",
        "\n",
        "  # Train\n",
        "  for batch_idx, samples in enumerate(dataloader['train']):\n",
        "    x_train, y_train = samples\n",
        "\n",
        "    pred = model(x_train)\n",
        "    loss = loss_fn(pred, y_train[:,0])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost+=loss\n",
        "    sample_size+=1\n",
        "\n",
        "  # Val\n",
        "  for batch_idx, samples in enumerate(dataloader['val']):\n",
        "    x_train, y_train = samples\n",
        "    pred = model(x_train)\n",
        "    loss = loss_fn(pred, y_train[:,0])\n",
        "    avg_cost_+=loss\n",
        "    sample_size_+=1\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  print(\"epoch :\" , epoch, \"loss_train\", avg_cost/sample_size, \"loss_val\", avg_cost_/sample_size_ )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 loss_train tensor(0.6451, grad_fn=<DivBackward0>) loss_val tensor(0.9618, grad_fn=<DivBackward0>)\n",
            "epoch : 1 loss_train tensor(0.5726, grad_fn=<DivBackward0>) loss_val tensor(1.1406, grad_fn=<DivBackward0>)\n",
            "epoch : 2 loss_train tensor(0.5532, grad_fn=<DivBackward0>) loss_val tensor(1.1214, grad_fn=<DivBackward0>)\n",
            "epoch : 3 loss_train tensor(0.5344, grad_fn=<DivBackward0>) loss_val tensor(1.0083, grad_fn=<DivBackward0>)\n",
            "epoch : 4 loss_train tensor(0.4997, grad_fn=<DivBackward0>) loss_val tensor(0.8300, grad_fn=<DivBackward0>)\n",
            "epoch : 5 loss_train tensor(0.4731, grad_fn=<DivBackward0>) loss_val tensor(0.8104, grad_fn=<DivBackward0>)\n",
            "epoch : 6 loss_train tensor(0.4508, grad_fn=<DivBackward0>) loss_val tensor(0.7264, grad_fn=<DivBackward0>)\n",
            "epoch : 7 loss_train tensor(0.4417, grad_fn=<DivBackward0>) loss_val tensor(0.7189, grad_fn=<DivBackward0>)\n",
            "epoch : 8 loss_train tensor(0.4312, grad_fn=<DivBackward0>) loss_val tensor(0.6874, grad_fn=<DivBackward0>)\n",
            "epoch : 9 loss_train tensor(0.4277, grad_fn=<DivBackward0>) loss_val tensor(0.6576, grad_fn=<DivBackward0>)\n",
            "epoch : 10 loss_train tensor(0.4203, grad_fn=<DivBackward0>) loss_val tensor(0.6993, grad_fn=<DivBackward0>)\n",
            "epoch : 11 loss_train tensor(0.4200, grad_fn=<DivBackward0>) loss_val tensor(0.6433, grad_fn=<DivBackward0>)\n",
            "epoch : 12 loss_train tensor(0.4111, grad_fn=<DivBackward0>) loss_val tensor(0.7165, grad_fn=<DivBackward0>)\n",
            "epoch : 13 loss_train tensor(0.4112, grad_fn=<DivBackward0>) loss_val tensor(0.7040, grad_fn=<DivBackward0>)\n",
            "epoch : 14 loss_train tensor(0.4091, grad_fn=<DivBackward0>) loss_val tensor(0.6978, grad_fn=<DivBackward0>)\n",
            "epoch : 15 loss_train tensor(0.4055, grad_fn=<DivBackward0>) loss_val tensor(0.6763, grad_fn=<DivBackward0>)\n",
            "epoch : 16 loss_train tensor(0.4058, grad_fn=<DivBackward0>) loss_val tensor(0.6713, grad_fn=<DivBackward0>)\n",
            "epoch : 17 loss_train tensor(0.4001, grad_fn=<DivBackward0>) loss_val tensor(0.7237, grad_fn=<DivBackward0>)\n",
            "epoch : 18 loss_train tensor(0.3991, grad_fn=<DivBackward0>) loss_val tensor(0.6489, grad_fn=<DivBackward0>)\n",
            "epoch : 19 loss_train tensor(0.4003, grad_fn=<DivBackward0>) loss_val tensor(0.6723, grad_fn=<DivBackward0>)\n",
            "epoch : 20 loss_train tensor(0.3955, grad_fn=<DivBackward0>) loss_val tensor(0.7098, grad_fn=<DivBackward0>)\n",
            "epoch : 21 loss_train tensor(0.3960, grad_fn=<DivBackward0>) loss_val tensor(0.6820, grad_fn=<DivBackward0>)\n",
            "epoch : 22 loss_train tensor(0.3936, grad_fn=<DivBackward0>) loss_val tensor(0.6856, grad_fn=<DivBackward0>)\n",
            "epoch : 23 loss_train tensor(0.3898, grad_fn=<DivBackward0>) loss_val tensor(0.7054, grad_fn=<DivBackward0>)\n",
            "epoch : 24 loss_train tensor(0.3927, grad_fn=<DivBackward0>) loss_val tensor(0.7151, grad_fn=<DivBackward0>)\n",
            "epoch : 25 loss_train tensor(0.3929, grad_fn=<DivBackward0>) loss_val tensor(0.6553, grad_fn=<DivBackward0>)\n",
            "epoch : 26 loss_train tensor(0.3911, grad_fn=<DivBackward0>) loss_val tensor(0.6765, grad_fn=<DivBackward0>)\n",
            "epoch : 27 loss_train tensor(0.3852, grad_fn=<DivBackward0>) loss_val tensor(0.6683, grad_fn=<DivBackward0>)\n",
            "epoch : 28 loss_train tensor(0.3841, grad_fn=<DivBackward0>) loss_val tensor(0.7262, grad_fn=<DivBackward0>)\n",
            "epoch : 29 loss_train tensor(0.3855, grad_fn=<DivBackward0>) loss_val tensor(0.6405, grad_fn=<DivBackward0>)\n",
            "epoch : 30 loss_train tensor(0.3824, grad_fn=<DivBackward0>) loss_val tensor(0.7110, grad_fn=<DivBackward0>)\n",
            "epoch : 31 loss_train tensor(0.3795, grad_fn=<DivBackward0>) loss_val tensor(0.6670, grad_fn=<DivBackward0>)\n",
            "epoch : 32 loss_train tensor(0.3821, grad_fn=<DivBackward0>) loss_val tensor(0.6447, grad_fn=<DivBackward0>)\n",
            "epoch : 33 loss_train tensor(0.3801, grad_fn=<DivBackward0>) loss_val tensor(0.6867, grad_fn=<DivBackward0>)\n",
            "epoch : 34 loss_train tensor(0.3795, grad_fn=<DivBackward0>) loss_val tensor(0.7236, grad_fn=<DivBackward0>)\n",
            "epoch : 35 loss_train tensor(0.3732, grad_fn=<DivBackward0>) loss_val tensor(0.6684, grad_fn=<DivBackward0>)\n",
            "epoch : 36 loss_train tensor(0.3725, grad_fn=<DivBackward0>) loss_val tensor(0.7128, grad_fn=<DivBackward0>)\n",
            "epoch : 37 loss_train tensor(0.3734, grad_fn=<DivBackward0>) loss_val tensor(0.6735, grad_fn=<DivBackward0>)\n",
            "epoch : 38 loss_train tensor(0.3741, grad_fn=<DivBackward0>) loss_val tensor(0.6656, grad_fn=<DivBackward0>)\n",
            "epoch : 39 loss_train tensor(0.3732, grad_fn=<DivBackward0>) loss_val tensor(0.7088, grad_fn=<DivBackward0>)\n",
            "epoch : 40 loss_train tensor(0.3752, grad_fn=<DivBackward0>) loss_val tensor(0.6996, grad_fn=<DivBackward0>)\n",
            "epoch : 41 loss_train tensor(0.3674, grad_fn=<DivBackward0>) loss_val tensor(0.6654, grad_fn=<DivBackward0>)\n",
            "epoch : 42 loss_train tensor(0.3755, grad_fn=<DivBackward0>) loss_val tensor(0.7110, grad_fn=<DivBackward0>)\n",
            "epoch : 43 loss_train tensor(0.3694, grad_fn=<DivBackward0>) loss_val tensor(0.6822, grad_fn=<DivBackward0>)\n",
            "epoch : 44 loss_train tensor(0.3706, grad_fn=<DivBackward0>) loss_val tensor(0.6515, grad_fn=<DivBackward0>)\n",
            "epoch : 45 loss_train tensor(0.3717, grad_fn=<DivBackward0>) loss_val tensor(0.7322, grad_fn=<DivBackward0>)\n",
            "epoch : 46 loss_train tensor(0.3749, grad_fn=<DivBackward0>) loss_val tensor(0.6719, grad_fn=<DivBackward0>)\n",
            "epoch : 47 loss_train tensor(0.3661, grad_fn=<DivBackward0>) loss_val tensor(0.6973, grad_fn=<DivBackward0>)\n",
            "epoch : 48 loss_train tensor(0.3643, grad_fn=<DivBackward0>) loss_val tensor(0.6733, grad_fn=<DivBackward0>)\n",
            "epoch : 49 loss_train tensor(0.3641, grad_fn=<DivBackward0>) loss_val tensor(0.6905, grad_fn=<DivBackward0>)\n",
            "epoch : 50 loss_train tensor(0.3633, grad_fn=<DivBackward0>) loss_val tensor(0.6850, grad_fn=<DivBackward0>)\n",
            "epoch : 51 loss_train tensor(0.3622, grad_fn=<DivBackward0>) loss_val tensor(0.6896, grad_fn=<DivBackward0>)\n",
            "epoch : 52 loss_train tensor(0.3606, grad_fn=<DivBackward0>) loss_val tensor(0.6792, grad_fn=<DivBackward0>)\n",
            "epoch : 53 loss_train tensor(0.3631, grad_fn=<DivBackward0>) loss_val tensor(0.6776, grad_fn=<DivBackward0>)\n",
            "epoch : 54 loss_train tensor(0.3597, grad_fn=<DivBackward0>) loss_val tensor(0.7022, grad_fn=<DivBackward0>)\n",
            "epoch : 55 loss_train tensor(0.3577, grad_fn=<DivBackward0>) loss_val tensor(0.7001, grad_fn=<DivBackward0>)\n",
            "epoch : 56 loss_train tensor(0.3563, grad_fn=<DivBackward0>) loss_val tensor(0.7030, grad_fn=<DivBackward0>)\n",
            "epoch : 57 loss_train tensor(0.3554, grad_fn=<DivBackward0>) loss_val tensor(0.6898, grad_fn=<DivBackward0>)\n",
            "epoch : 58 loss_train tensor(0.3544, grad_fn=<DivBackward0>) loss_val tensor(0.6913, grad_fn=<DivBackward0>)\n",
            "epoch : 59 loss_train tensor(0.3562, grad_fn=<DivBackward0>) loss_val tensor(0.6972, grad_fn=<DivBackward0>)\n",
            "epoch : 60 loss_train tensor(0.3562, grad_fn=<DivBackward0>) loss_val tensor(0.6830, grad_fn=<DivBackward0>)\n",
            "epoch : 61 loss_train tensor(0.3573, grad_fn=<DivBackward0>) loss_val tensor(0.6946, grad_fn=<DivBackward0>)\n",
            "epoch : 62 loss_train tensor(0.3565, grad_fn=<DivBackward0>) loss_val tensor(0.6635, grad_fn=<DivBackward0>)\n",
            "epoch : 63 loss_train tensor(0.3560, grad_fn=<DivBackward0>) loss_val tensor(0.7015, grad_fn=<DivBackward0>)\n",
            "epoch : 64 loss_train tensor(0.3521, grad_fn=<DivBackward0>) loss_val tensor(0.6888, grad_fn=<DivBackward0>)\n",
            "epoch : 65 loss_train tensor(0.3524, grad_fn=<DivBackward0>) loss_val tensor(0.6960, grad_fn=<DivBackward0>)\n",
            "epoch : 66 loss_train tensor(0.3515, grad_fn=<DivBackward0>) loss_val tensor(0.6933, grad_fn=<DivBackward0>)\n",
            "epoch : 67 loss_train tensor(0.3508, grad_fn=<DivBackward0>) loss_val tensor(0.6953, grad_fn=<DivBackward0>)\n",
            "epoch : 68 loss_train tensor(0.3506, grad_fn=<DivBackward0>) loss_val tensor(0.6982, grad_fn=<DivBackward0>)\n",
            "epoch : 69 loss_train tensor(0.3510, grad_fn=<DivBackward0>) loss_val tensor(0.6901, grad_fn=<DivBackward0>)\n",
            "epoch : 70 loss_train tensor(0.3506, grad_fn=<DivBackward0>) loss_val tensor(0.6831, grad_fn=<DivBackward0>)\n",
            "epoch : 71 loss_train tensor(0.3509, grad_fn=<DivBackward0>) loss_val tensor(0.6910, grad_fn=<DivBackward0>)\n",
            "epoch : 72 loss_train tensor(0.3502, grad_fn=<DivBackward0>) loss_val tensor(0.7035, grad_fn=<DivBackward0>)\n",
            "epoch : 73 loss_train tensor(0.3496, grad_fn=<DivBackward0>) loss_val tensor(0.6806, grad_fn=<DivBackward0>)\n",
            "epoch : 74 loss_train tensor(0.3496, grad_fn=<DivBackward0>) loss_val tensor(0.6995, grad_fn=<DivBackward0>)\n",
            "epoch : 75 loss_train tensor(0.3499, grad_fn=<DivBackward0>) loss_val tensor(0.6934, grad_fn=<DivBackward0>)\n",
            "epoch : 76 loss_train tensor(0.3488, grad_fn=<DivBackward0>) loss_val tensor(0.6942, grad_fn=<DivBackward0>)\n",
            "epoch : 77 loss_train tensor(0.3487, grad_fn=<DivBackward0>) loss_val tensor(0.6954, grad_fn=<DivBackward0>)\n",
            "epoch : 78 loss_train tensor(0.3485, grad_fn=<DivBackward0>) loss_val tensor(0.6900, grad_fn=<DivBackward0>)\n",
            "epoch : 79 loss_train tensor(0.3486, grad_fn=<DivBackward0>) loss_val tensor(0.7000, grad_fn=<DivBackward0>)\n",
            "epoch : 80 loss_train tensor(0.3489, grad_fn=<DivBackward0>) loss_val tensor(0.6923, grad_fn=<DivBackward0>)\n",
            "epoch : 81 loss_train tensor(0.3482, grad_fn=<DivBackward0>) loss_val tensor(0.6995, grad_fn=<DivBackward0>)\n",
            "epoch : 82 loss_train tensor(0.3496, grad_fn=<DivBackward0>) loss_val tensor(0.6909, grad_fn=<DivBackward0>)\n",
            "epoch : 83 loss_train tensor(0.3489, grad_fn=<DivBackward0>) loss_val tensor(0.6832, grad_fn=<DivBackward0>)\n",
            "epoch : 84 loss_train tensor(0.3491, grad_fn=<DivBackward0>) loss_val tensor(0.6870, grad_fn=<DivBackward0>)\n",
            "epoch : 85 loss_train tensor(0.3481, grad_fn=<DivBackward0>) loss_val tensor(0.6950, grad_fn=<DivBackward0>)\n",
            "epoch : 86 loss_train tensor(0.3480, grad_fn=<DivBackward0>) loss_val tensor(0.6990, grad_fn=<DivBackward0>)\n",
            "epoch : 87 loss_train tensor(0.3479, grad_fn=<DivBackward0>) loss_val tensor(0.6894, grad_fn=<DivBackward0>)\n",
            "epoch : 88 loss_train tensor(0.3485, grad_fn=<DivBackward0>) loss_val tensor(0.7077, grad_fn=<DivBackward0>)\n",
            "epoch : 89 loss_train tensor(0.3506, grad_fn=<DivBackward0>) loss_val tensor(0.6716, grad_fn=<DivBackward0>)\n",
            "epoch : 90 loss_train tensor(0.3481, grad_fn=<DivBackward0>) loss_val tensor(0.6949, grad_fn=<DivBackward0>)\n",
            "epoch : 91 loss_train tensor(0.3478, grad_fn=<DivBackward0>) loss_val tensor(0.6948, grad_fn=<DivBackward0>)\n",
            "epoch : 92 loss_train tensor(0.3476, grad_fn=<DivBackward0>) loss_val tensor(0.6819, grad_fn=<DivBackward0>)\n",
            "epoch : 93 loss_train tensor(0.3475, grad_fn=<DivBackward0>) loss_val tensor(0.6914, grad_fn=<DivBackward0>)\n",
            "epoch : 94 loss_train tensor(0.3474, grad_fn=<DivBackward0>) loss_val tensor(0.6962, grad_fn=<DivBackward0>)\n",
            "epoch : 95 loss_train tensor(0.3472, grad_fn=<DivBackward0>) loss_val tensor(0.6790, grad_fn=<DivBackward0>)\n",
            "epoch : 96 loss_train tensor(0.3476, grad_fn=<DivBackward0>) loss_val tensor(0.6924, grad_fn=<DivBackward0>)\n",
            "epoch : 97 loss_train tensor(0.3472, grad_fn=<DivBackward0>) loss_val tensor(0.6893, grad_fn=<DivBackward0>)\n",
            "epoch : 98 loss_train tensor(0.3472, grad_fn=<DivBackward0>) loss_val tensor(0.6748, grad_fn=<DivBackward0>)\n",
            "epoch : 99 loss_train tensor(0.3473, grad_fn=<DivBackward0>) loss_val tensor(0.6918, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    }
  ]
}